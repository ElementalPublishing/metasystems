{
  "system": "roboter",
  "description": "Entry point for roboter as a meta system.\nUsage: main(url)\nExample: main(\"example.com\")",
  "detected_inputs": [
    {
      "type": "input",
      "description": "User input requested: \nPress Enter to exit..."
    },
    {
      "type": "input",
      "description": "User input requested: Enter a website URL (any page or domain): "
    }
  ],
  "detected_outputs": [
    {
      "type": "console",
      "description": "Prints output to the terminal"
    }
  ],
  "source_code": "import requests\nfrom urllib.parse import urlparse\nimport socket\nimport os\nimport re\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport ast\n\nCOMMON_SUBDOMAINS = [\n    \"www\", \"blog\", \"mail\", \"api\", \"dev\", \"test\", \"shop\", \"forum\", \"news\", \"m\"\n]\n\nMY_USER_AGENT = \"roboter\"\n\nclass RobotsRules:\n    def __init__(self):\n        self.disallow = []\n        self.allow = []\n        self.crawl_delay = None\n\n    def is_path_allowed(self, path):\n        # If any Allow matches, it's allowed, unless a more specific Disallow matches\n        allowed = False\n        for rule in self.allow:\n            if path.startswith(rule):\n                allowed = True\n        for rule in self.disallow:\n            if path.startswith(rule):\n                allowed = False\n        return allowed if self.allow or self.disallow else True\n\nclass Roboter:\n    def __init__(self, url):\n        # Parse the input URL and construct the root domain\n        parsed = urlparse(url if url.startswith(\"http\") else \"https://\" + url)\n        self.root_domain = parsed.netloc or parsed.path\n        self.scheme = parsed.scheme or \"https\"\n        self.root_url = f\"{self.scheme}://{self.root_domain}\".rstrip(\"/\")\n        self.rules = {}\n        self.found_subdomains = set()\n        self.robots_parsed = {}\n\n    def fetch_robots(self, domain):\n        robots_url = f\"{self.scheme}://{domain}/robots.txt\"\n        try:\n            response = requests.get(robots_url, timeout=10)\n            response.raise_for_status()\n            self.rules[domain] = response.text\n            self.robots_parsed[domain] = self.parse_robots(response.text)\n            return response.text\n        except requests.RequestException:\n            self.rules[domain] = \"\"\n            self.robots_parsed[domain] = None\n            return \"\"\n\n    def parse_robots(self, robots_txt):\n        # Parse robots.txt for our user-agent and *\n        if not robots_txt:\n            return None\n        groups = re.split(r'(?i)^User-agent:', robots_txt, flags=re.MULTILINE)\n        relevant = []\n        for group in groups:\n            lines = group.strip().splitlines()\n            if not lines:\n                continue\n            agent = lines[0].strip().lower()\n            if agent == \"*\" or agent == MY_USER_AGENT.lower():\n                relevant.append(lines[1:])\n        # Merge all relevant groups\n        rules = RobotsRules()\n        for lines in relevant:\n            for line in lines:\n                if line.lower().startswith(\"disallow:\"):\n                    path = line.split(\":\", 1)[1].strip()\n                    if path:\n                        rules.disallow.append(path)\n                elif line.lower().startswith(\"allow:\"):\n                    path = line.split(\":\", 1)[1].strip()\n                    if path:\n                        rules.allow.append(path)\n                elif line.lower().startswith(\"crawl-delay:\"):\n                    try:\n                        delay = float(line.split(\":\", 1)[1].strip())\n                        rules.crawl_delay = delay\n                    except Exception:\n                        pass\n        return rules\n\n    def find_subdomains(self):\n        found = set()\n        for sub in COMMON_SUBDOMAINS:\n            subdomain = f\"{sub}.{self.root_domain}\"\n            try:\n                socket.gethostbyname(subdomain)\n                found.add(subdomain)\n            except socket.gaierror:\n                continue\n        self.found_subdomains = found\n        return found\n\n    def can_crawl(self, domain, path=\"/\"):\n        rules = self.robots_parsed.get(domain)\n        if not rules:\n            return True  # No robots.txt, so allowed\n        return rules.is_path_allowed(path)\n\n    def get_crawl_delay(self, domain):\n        rules = self.robots_parsed.get(domain)\n        if rules and rules.crawl_delay is not None:\n            return rules.crawl_delay\n        return 0\n\n    def print_rules(self):\n        start_time = time.time()\n        # 1. Always check the root domain FIRST\n        robots_txt = self.fetch_robots(self.root_domain)\n        if not self.can_crawl(self.root_domain, \"/\"):\n            print(f\"\\nCrawling is disallowed for user-agent '{MY_USER_AGENT}' or all bots on {self.root_domain}. Aborting.\")\n            print(f\"Time elapsed: {time.time() - start_time:.2f} seconds\")\n            return\n\n        crawl_delay = self.get_crawl_delay(self.root_domain)\n        if crawl_delay:\n            print(f\"Crawl-delay for {self.root_domain}: {crawl_delay} seconds\")\n            print(f\"Respecting crawl-delay: Waiting {crawl_delay} seconds before continuing...\")\n            time.sleep(crawl_delay)\n\n        # 2. Only after root, check discovered subdomains in parallel\n        subdomains = self.find_subdomains()\n\n        def process_sub(sub):\n            robots_txt_sub = self.fetch_robots(sub)\n            if not self.can_crawl(sub, \"/\"):\n                print(f\"\\nCrawling is disallowed for user-agent '{MY_USER_AGENT}' or all bots on {sub}. Skipping.\")\n                return\n            sub_crawl_delay = self.get_crawl_delay(sub)\n            if sub_crawl_delay:\n                print(f\"Crawl-delay for {sub}: {sub_crawl_delay} seconds\")\n                print(f\"Respecting crawl-delay: Waiting {sub_crawl_delay} seconds before continuing...\")\n                time.sleep(sub_crawl_delay)\n\n        if subdomains:\n            with ThreadPoolExecutor(max_workers=5) as executor:\n                futures = [executor.submit(process_sub, sub) for sub in sorted(subdomains)]\n                for future in as_completed(futures):\n                    pass  # Output is handled in process_sub\n\n        self.write_markdown()\n        elapsed = time.time() - start_time\n        print(f\"\\nTime elapsed: {elapsed:.2f} seconds\")\n\n    def write_markdown(self):\n        filename = f\"{self.root_domain.replace('.', '_')}_robots.md\"\n        with open(filename, \"w\", encoding=\"utf-8\") as f:\n            f.write(f\"# robots.txt Results for {self.root_domain}\\n\\n\")\n            f.write(f\"## Root Domain: {self.root_domain}\\n\")\n            if self.rules[self.root_domain]:\n                f.write(f\"### robots.txt\\n```\\n{self.rules[self.root_domain]}\\n```\\n\")\n            else:\n                f.write(\"No robots.txt found for root domain.\\n\")\n            if self.found_subdomains:\n                f.write(\"\\n## Subdomains Found:\\n\")\n                for sub in sorted(self.found_subdomains):\n                    f.write(f\"\\n### {sub}\\n\")\n                    if self.rules[sub]:\n                        f.write(f\"```\\n{self.rules[sub]}\\n```\\n\")\n                    else:\n                        f.write(\"No robots.txt found.\\n\")\n            else:\n                f.write(\"\\nNo common subdomains found.\\n\")\n        print(f\"\\nResults written to {filename}\")\n\n    def analyze_outputs(filepath):\n        outputs = []\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n            source = f.read()\n        tree = ast.parse(source, filename=filepath)\n        for node in ast.walk(tree):\n            # Detect file writes\n            if isinstance(node, ast.Call) and hasattr(node.func, 'id') and node.func.id == 'open':\n                if len(node.args) >= 2 and hasattr(node.args[1], 's'):\n                    mode = node.args[1].s\n                    if 'w' in mode:\n                        if hasattr(node.args[0], 's'):\n                            outputs.append({\n                                \"type\": \"file\",\n                                \"filename\": node.args[0].s,\n                                \"mode\": mode,\n                                \"description\": \"File written by program\"\n                            })\n            # Detect print statements\n            if isinstance(node, ast.Call) and hasattr(node.func, 'id') and node.func.id == 'print':\n                outputs.append({\n                    \"type\": \"console\",\n                    \"description\": \"Prints output to the terminal\"\n                })\n        return outputs\n\ndef main(*args):\n    \"\"\"\n    Entry point for roboter as a meta system.\n    Usage: main(url)\n    Example: main(\"example.com\")\n    \"\"\"\n    if not args:\n        print(\"Usage: roboter <website_url>\")\n        return\n    url = args[0]\n    roboter = Roboter(url)\n    roboter.print_rules()\n\nif __name__ == \"__main__\":\n    print(\"NOTICE: This tool checks robots.txt and will not crawl if your user-agent or all bots are disallowed.\")\n    print(\"It also respects crawl-delay and path rules. You are responsible for complying with all applicable laws and the website's Terms of Service.\\n\")\n    url = input(\"Enter a website URL (any page or domain): \").strip()\n    roboter = Roboter(url)\n    roboter.print_rules()\n    input(\"\\nPress Enter to exit...\")",
  "tags": [],
  "dependencies": [
    "time",
    "os",
    "requests",
    "concurrent.futures",
    "ast",
    "urllib.parse",
    "socket",
    "re"
  ],
  "status": "idle",
  "config": {},
  "result": null,
  "documentation": "",
  "created": null,
  "last_run": null,
  "author": "",
  "visualization_hints": {},
  "permissions": [],
  "metrics": {}
}